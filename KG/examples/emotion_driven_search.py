#!/usr/bin/env python3
"""
æƒ…ç»ªé©±åŠ¨éŸ³ä¹æ£€ç´¢ç¤ºä¾‹

å±•ç¤ºå¦‚ä½•ä½¿ç”¨KGæ¨¡å—è¿›è¡Œå®Œæ•´çš„æƒ…ç»ªé©±åŠ¨éŸ³ä¹æ²»ç–—æµç¨‹
åŒ…å«å¤šç§ä½¿ç”¨åœºæ™¯å’Œæœ€ä½³å®è·µ
"""

import sys
import os
import numpy as np
from pathlib import Path

# æ·»åŠ KGæ¨¡å—è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from knowledge_graph import KnowledgeGraph
from emotion_music_bridge import EmotionMusicBridge
from parameter_mapping import ParameterMapper

def example_1_basic_usage():
    """ç¤ºä¾‹1: åŸºç¡€ä½¿ç”¨æµç¨‹"""
    print("ğŸŒŸ ç¤ºä¾‹1: åŸºç¡€æƒ…ç»ªåˆ†æä¸éŸ³ä¹æ¨è")
    print("-" * 50)
    
    # åˆå§‹åŒ–æ¡¥æ¥å™¨
    bridge = EmotionMusicBridge(enable_mi_retrieve=True)
    
    # å®šä¹‰ç”¨æˆ·æƒ…ç»ªçŠ¶æ€
    user_emotions = {
        "ç„¦è™‘": 0.8,     # é«˜åº¦ç„¦è™‘
        "å¹³é™": 0.1,     # ä½å¹³é™åº¦  
        "ææƒ§": 0.3      # è½»å¾®ææƒ§
    }
    
    print(f"ğŸ§  ç”¨æˆ·æƒ…ç»ªçŠ¶æ€: {user_emotions}")
    
    # åˆ›å»ºæƒ…ç»ªå‘é‡
    emotion_vector = bridge.create_emotion_vector_from_dict(user_emotions)
    
    # è·å–éŸ³ä¹æ¨è
    result = bridge.analyze_emotion_and_recommend_music(emotion_vector, duration="3min", top_k=3)
    
    if result["success"]:
        print(f"âœ… åˆ†ææˆåŠŸ")
        print(f"ä¸»è¦æƒ…ç»ª: {result['emotion_analysis']['max_emotion']}")
        print(f"éŸ³ä¹å‚æ•°: {result['music_parameters']}")
        print(f"æ²»ç–—å»ºè®®: {result['therapy_recommendation']['primary_focus']}")
        
        if result["music_search_results"] and result["music_search_results"]["success"]:
            print(f"æ¨èéŸ³ä¹: {len(result['music_search_results']['results'])} é¦–")
        else:
            print("éŸ³ä¹æ£€ç´¢ä¸å¯ç”¨")
    else:
        print(f"âŒ åˆ†æå¤±è´¥: {result['error']}")
    
    return result

def example_2_detailed_analysis():
    """ç¤ºä¾‹2: è¯¦ç»†æƒ…ç»ªåˆ†æ"""
    print("\nğŸŒŸ ç¤ºä¾‹2: è¯¦ç»†æƒ…ç»ªåˆ†æä¸å‚æ•°æ˜ å°„")
    print("-" * 50)
    
    # åˆå§‹åŒ–ç»„ä»¶
    kg = KnowledgeGraph()
    mapper = ParameterMapper()
    
    # å¤æ‚æƒ…ç»ªçŠ¶æ€
    complex_emotions = {
        "æ‚²ä¼¤": 0.7,
        "æ€€æ—§": 0.5,
        "å¹³é™": 0.3,
        "å®¡ç¾æ¬£èµ": 0.4
    }
    
    print(f"ğŸ­ å¤æ‚æƒ…ç»ªçŠ¶æ€: {complex_emotions}")
    
    # åˆ›å»ºæƒ…ç»ªå‘é‡
    emotion_vector = np.zeros(27)
    for emotion_name, value in complex_emotions.items():
        if emotion_name in kg.emotion_names:
            index = kg.emotion_names.index(emotion_name)
            emotion_vector[index] = value
    
    # è¯¦ç»†åˆ†æ
    emotion_analysis = kg.analyze_emotion_vector(emotion_vector)
    print(f"\nğŸ“Š è¯¦ç»†æƒ…ç»ªåˆ†æ:")
    print(f"   å‰3æƒ…ç»ª: {emotion_analysis['top_emotions'][:3]}")
    print(f"   æƒ…ç»ªå¹³è¡¡: {emotion_analysis['emotion_balance']}")
    print(f"   æ•´ä½“å¼ºåº¦: {emotion_analysis['overall_intensity']:.3f}")
    print(f"   æƒ…ç»ªå¤šæ ·æ€§: {emotion_analysis['emotion_diversity']}")
    
    # è·å–éŸ³ä¹å‚æ•°
    music_params = kg.get_initial_music_parameters(emotion_vector)
    print(f"\nğŸµ éŸ³ä¹å‚æ•°:")
    for key, value in music_params.items():
        print(f"   {key}: {value}")
    
    # å‚æ•°æ˜ å°„
    text_desc = mapper.kg_to_text_description(music_params)
    structured_params = mapper.kg_to_structured_params(music_params)
    
    print(f"\nğŸ“ è‡ªç„¶è¯­è¨€æè¿°:")
    print(f"   {text_desc}")
    
    print(f"\nğŸ—ï¸  ç»“æ„åŒ–å‚æ•°:")
    for key, value in structured_params.items():
        print(f"   {key}: {value}")

def example_3_therapy_scenarios():
    """ç¤ºä¾‹3: ä¸åŒæ²»ç–—åœºæ™¯"""
    print("\nğŸŒŸ ç¤ºä¾‹3: ä¸åŒéŸ³ä¹æ²»ç–—åœºæ™¯")
    print("-" * 50)
    
    bridge = EmotionMusicBridge(enable_mi_retrieve=True)
    
    # å®šä¹‰å¤šç§æ²»ç–—åœºæ™¯
    therapy_scenarios = [
        {
            "name": "è€ƒè¯•ç„¦è™‘ç¼“è§£",
            "emotions": {"ç„¦è™‘": 0.8, "ææƒ§": 0.4, "å¹³é™": 0.1},
            "goal": "é™ä½ç„¦è™‘ï¼Œæå‡ä¸“æ³¨åŠ›"
        },
        {
            "name": "å¤±æ‹æƒ…æ„Ÿæ”¯æŒ",
            "emotions": {"æ‚²ä¼¤": 0.9, "å¤±æœ›": 0.7, "æ„¤æ€’": 0.3, "æ€€æ—§": 0.5},
            "goal": "æƒ…æ„Ÿå®£æ³„ï¼Œé€æ­¥æ„ˆåˆ"
        },
        {
            "name": "å·¥ä½œå‹åŠ›é‡Šæ”¾",
            "emotions": {"æ„¤æ€’": 0.6, "ç„¦è™‘": 0.5, "ç–²åŠ³": 0.8},  # æ³¨æ„: ç–²åŠ³ä¸åœ¨27ç»´ä¸­
            "goal": "å‹åŠ›é‡Šæ”¾ï¼Œèº«å¿ƒæ”¾æ¾"
        },
        {
            "name": "åº†ç¥æˆåŠŸå–œæ‚¦",
            "emotions": {"å¿«ä¹": 0.9, "å…´å¥‹": 0.8, "é’¦ä½©": 0.6},
            "goal": "ç»´æŒç§¯ææƒ…ç»ªï¼Œåˆ†äº«å–œæ‚¦"
        },
        {
            "name": "æ·±åº¦å†¥æƒ³",
            "emotions": {"å¹³é™": 0.9, "å®¡ç¾æ¬£èµ": 0.7, "æ•¬ç•": 0.5},
            "goal": "æ·±å±‚æ”¾æ¾ï¼Œå†…åœ¨æ¢ç´¢"
        }
    ]
    
    for i, scenario in enumerate(therapy_scenarios, 1):
        print(f"\nğŸ¬ åœºæ™¯{i}: {scenario['name']}")
        print(f"   æ²»ç–—ç›®æ ‡: {scenario['goal']}")
        print(f"   æƒ…ç»ªçŠ¶æ€: {scenario['emotions']}")
        
        # åˆ†ææƒ…ç»ªå¹¶è·å–æ²»ç–—å»ºè®®
        emotion_vector = bridge.create_emotion_vector_from_dict(scenario["emotions"])
        result = bridge.get_therapy_parameters_only(emotion_vector)
        
        if result["success"]:
            therapy = result["therapy_recommendation"]
            music = result["music_parameters"]
            
            print(f"   ğŸ’Š æ²»ç–—å»ºè®®:")
            print(f"      ç„¦ç‚¹: {therapy['primary_focus']}")
            print(f"      æ–¹æ³•: {therapy['therapy_approach']}")
            print(f"      æ—¶é•¿: {therapy['session_duration']}")
            
            print(f"   ğŸµ éŸ³ä¹ç‰¹å¾:")
            print(f"      èŠ‚æ‹: {music['tempo']} BPM")
            print(f"      è°ƒå¼: {music['mode']}")
            print(f"      éŸ³è‰²: {music['timbre']}")
        else:
            print(f"   âŒ åˆ†æå¤±è´¥: {result['error']}")

def example_4_batch_processing():
    """ç¤ºä¾‹4: æ‰¹é‡å¤„ç†"""
    print("\nğŸŒŸ ç¤ºä¾‹4: æ‰¹é‡æƒ…ç»ªåˆ†æ")
    print("-" * 50)
    
    bridge = EmotionMusicBridge(enable_mi_retrieve=False)  # ä»…å‚æ•°æ¨¡å¼ï¼Œé¿å…è¿‡å¤šæ£€ç´¢
    
    # åˆ›å»ºå¤šä¸ªæƒ…ç»ªå‘é‡
    emotion_sets = [
        {"å¿«ä¹": 0.8, "å…´å¥‹": 0.6},
        {"æ‚²ä¼¤": 0.7, "å¤±æœ›": 0.5},
        {"ç„¦è™‘": 0.9, "ææƒ§": 0.4},
        {"å¹³é™": 0.8, "å®¡ç¾æ¬£èµ": 0.6},
        {"æ„¤æ€’": 0.7, "åŒæ¶": 0.4}
    ]
    
    emotion_vectors = []
    for emotions in emotion_sets:
        vector = bridge.create_emotion_vector_from_dict(emotions)
        emotion_vectors.append(vector)
    
    print(f"ğŸ“¦ æ‰¹é‡å¤„ç† {len(emotion_vectors)} ä¸ªæƒ…ç»ªçŠ¶æ€...")
    
    # æ‰¹é‡åˆ†æ
    results = bridge.batch_emotion_analysis(emotion_vectors, duration="3min")
    
    print(f"\nğŸ“Š æ‰¹é‡åˆ†æç»“æœ:")
    for i, result in enumerate(results, 1):
        if result["success"]:
            max_emotion = result["emotion_analysis"]["max_emotion"]
            tempo = result["music_parameters"]["tempo"]
            print(f"   {i}. ä¸»è¦æƒ…ç»ª: {max_emotion[0]} ({max_emotion[1]:.2f}) -> Tempo: {tempo} BPM")
        else:
            print(f"   {i}. âŒ åˆ†æå¤±è´¥")

def example_5_advanced_usage():
    """ç¤ºä¾‹5: é«˜çº§ä½¿ç”¨åœºæ™¯"""
    print("\nğŸŒŸ ç¤ºä¾‹5: é«˜çº§ä½¿ç”¨åœºæ™¯ä¸è‡ªå®šä¹‰")
    print("-" * 50)
    
    kg = KnowledgeGraph()
    
    # æƒ…ç»ªå‘é‡æ’å€¼ (ä¸¤ç§æƒ…ç»ªçŠ¶æ€ä¹‹é—´çš„è¿‡æ¸¡)
    emotion_start = np.zeros(27)
    emotion_start[5] = 0.8  # ç„¦è™‘
    
    emotion_end = np.zeros(27) 
    emotion_end[9] = 0.8    # å¹³é™
    
    print("ğŸ”„ æƒ…ç»ªçŠ¶æ€è¿‡æ¸¡åˆ†æ:")
    print("   èµ·å§‹çŠ¶æ€: é«˜ç„¦è™‘")
    print("   ç›®æ ‡çŠ¶æ€: é«˜å¹³é™")
    
    # ç”Ÿæˆè¿‡æ¸¡åºåˆ—
    steps = 5
    for i in range(steps + 1):
        t = i / steps
        transition_vector = (1 - t) * emotion_start + t * emotion_end
        
        music_params = kg.get_initial_music_parameters(transition_vector)
        
        print(f"   æ­¥éª¤{i+1}: ç„¦è™‘{(1-t)*0.8:.1f}/å¹³é™{t*0.8:.1f} -> Tempo: {music_params['tempo']:.0f} BPM")
    
    # æƒ…ç»ªå‘é‡åˆ†æ
    print(f"\nğŸ” æƒ…ç»ªå‘é‡ç»Ÿè®¡åˆ†æ:")
    
    # åˆ›å»ºéšæœºæƒ…ç»ªçŠ¶æ€æ ·æœ¬
    np.random.seed(42)  # å¯é‡å¤æ€§
    random_vectors = []
    
    for _ in range(10):
        # éšæœºé€‰æ‹©2-4ä¸ªæƒ…ç»ªï¼Œç»™äºˆéšæœºå¼ºåº¦
        vector = np.zeros(27)
        num_emotions = np.random.randint(2, 5)
        selected_indices = np.random.choice(27, num_emotions, replace=False)
        
        for idx in selected_indices:
            vector[idx] = np.random.uniform(0.2, 1.0)
        
        random_vectors.append(vector)
    
    # åˆ†æéšæœºå‘é‡çš„æ¨¡å¼
    tempos = []
    modes = []
    
    for vector in random_vectors:
        params = kg.get_initial_music_parameters(vector)
        tempos.append(params['tempo'])
        modes.append(params['mode'])
    
    print(f"   éšæœºæ ·æœ¬æ•°: {len(random_vectors)}")
    print(f"   TempoèŒƒå›´: {min(tempos):.0f} - {max(tempos):.0f} BPM")
    print(f"   å¹³å‡Tempo: {np.mean(tempos):.0f} BPM")
    print(f"   ModeèŒƒå›´: {min(modes):.2f} - {max(modes):.2f}")
    print(f"   å¹³å‡Mode: {np.mean(modes):.2f}")

def example_6_error_handling():
    """ç¤ºä¾‹6: é”™è¯¯å¤„ç†ä¸å¼‚å¸¸æƒ…å†µ"""
    print("\nğŸŒŸ ç¤ºä¾‹6: é”™è¯¯å¤„ç†ä¸è¾¹ç•Œæƒ…å†µ")
    print("-" * 50)
    
    bridge = EmotionMusicBridge(enable_mi_retrieve=False)
    
    # æµ‹è¯•å„ç§å¼‚å¸¸è¾“å…¥
    test_cases = [
        {
            "name": "ç©ºæƒ…ç»ªå­—å…¸",
            "emotions": {},
            "expected": "åº”è¿”å›é»˜è®¤å‚æ•°"
        },
        {
            "name": "æœªçŸ¥æƒ…ç»ªåç§°",
            "emotions": {"å¼€å¿ƒ": 0.8, "ç”Ÿæ°”": 0.6},  # ä¸åœ¨27ç»´ä¸­
            "expected": "åº”å¿½ç•¥æœªçŸ¥æƒ…ç»ª"
        },
        {
            "name": "è¶…å‡ºèŒƒå›´çš„å€¼",
            "emotions": {"å¿«ä¹": 1.5, "æ‚²ä¼¤": -0.3},
            "expected": "åº”è£å‰ªåˆ°[0,1]èŒƒå›´"
        },
        {
            "name": "æå°å€¼æƒ…ç»ª",
            "emotions": {"å¿«ä¹": 0.001, "æ‚²ä¼¤": 0.002},
            "expected": "åº”æ­£å¸¸å¤„ç†"
        }
    ]
    
    for case in test_cases:
        print(f"\nğŸ§ª æµ‹è¯•: {case['name']}")
        print(f"   è¾“å…¥: {case['emotions']}")
        print(f"   æœŸæœ›: {case['expected']}")
        
        try:
            emotion_vector = bridge.create_emotion_vector_from_dict(case["emotions"])
            result = bridge.get_therapy_parameters_only(emotion_vector)
            
            if result["success"]:
                tempo = result["music_parameters"]["tempo"]
                primary_focus = result["therapy_recommendation"]["primary_focus"]
                print(f"   âœ… å¤„ç†æˆåŠŸ: Tempo={tempo} BPM, ç„¦ç‚¹={primary_focus}")
            else:
                print(f"   âš ï¸  å¤„ç†å¤±è´¥: {result['error']}")
                
        except Exception as e:
            print(f"   âŒ å¼‚å¸¸: {e}")

def main():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("ğŸµ æƒ…ç»ªé©±åŠ¨éŸ³ä¹æ£€ç´¢å®Œæ•´ç¤ºä¾‹")
    print("=" * 80)
    
    examples = [
        example_1_basic_usage,
        example_2_detailed_analysis,
        example_3_therapy_scenarios,
        example_4_batch_processing,
        example_5_advanced_usage,
        example_6_error_handling
    ]
    
    for example in examples:
        try:
            example()
        except Exception as e:
            print(f"âŒ ç¤ºä¾‹ {example.__name__} æ‰§è¡Œå¤±è´¥: {e}")
    
    print("\n" + "=" * 80)
    print("ğŸ‰ æ‰€æœ‰ç¤ºä¾‹æ¼”ç¤ºå®Œæˆ!")
    print("\nğŸ’¡ ä½¿ç”¨æç¤º:")
    print("   1. æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©åˆé€‚çš„æƒ…ç»ªåˆ†ææ¨¡å¼")
    print("   2. å¯ç”¨MI_retrieveå¯è·å¾—çœŸå®éŸ³ä¹æ¨è") 
    print("   3. æ²»ç–—å»ºè®®åº”ç»“åˆä¸“ä¸šå¿ƒç†è¯„ä¼°")
    print("   4. æ‰¹é‡å¤„ç†é€‚åˆç ”ç©¶å’Œç»Ÿè®¡åˆ†æ")
    print("   5. æ³¨æ„æƒ…ç»ªå‘é‡çš„27ç»´æ ‡å‡†æ ¼å¼")

if __name__ == "__main__":
    main()