#!/usr/bin/env python3
"""
æ£€ç´¢å¼•æ“æ¨¡å— - 4.0ç‰ˆæœ¬æ ¸å¿ƒç»„ä»¶
åŸºäºæƒ…ç»ªå’ŒéŸ³ä¹ç‰¹å¾è¿›è¡Œè§†é¢‘ç‰‡æ®µæ£€ç´¢åŒ¹é…
"""

import json
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import logging
import random

from .emotion_mapper import emotion_recognizer, music_feature_mapper

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class VideoRetrievalEngine:
    """
    è§†é¢‘æ£€ç´¢å¼•æ“
    åŸºäºæƒ…ç»ªç‰¹å¾å’ŒéŸ³ä¹ç‰¹å¾è¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—å’Œæ£€ç´¢
    """
    
    def __init__(self, features_dir: str = "materials/features"):
        """
        åˆå§‹åŒ–æ£€ç´¢å¼•æ“
        
        Args:
            features_dir: ç‰¹å¾æ•°æ®ç›®å½•
        """
        self.features_dir = Path(features_dir)
        self.features_database = {}
        self.emotion_database = {}
        
        # åŠ è½½æ•°æ®
        self.load_databases()
        
        # ç‰¹å¾æƒé‡é…ç½®
        self.feature_weights = {
            # åŸºç¡€ç‰¹å¾æƒé‡
            'tempo': 0.25,           # èŠ‚æ‹é‡è¦æ€§é«˜
            'energy': 0.20,          # èƒ½é‡é‡è¦æ€§é«˜  
            'brightness': 0.15,      # äº®åº¦
            'warmth': 0.15,          # æ¸©æš–åº¦
            'rhythm': 0.10,          # èŠ‚å¥è§„å¾‹æ€§
            'spectral': 0.10,        # é¢‘è°±ç‰¹å¾
            'dynamics': 0.05         # åŠ¨æ€èŒƒå›´
        }
        
        # æƒ…ç»ªç‰¹å¾æ˜ å°„æƒé‡
        self.emotion_weights = {
            'matching_stage': 1.0,   # ISOåŒ¹é…é˜¶æ®µæƒé‡æœ€é«˜
            'mood_similarity': 0.8,  # æƒ…ç»ªç›¸ä¼¼åº¦
            'energy_level': 0.6      # èƒ½é‡æ°´å¹³åŒ¹é…
        }
    
    def load_databases(self):
        """åŠ è½½ç‰¹å¾æ•°æ®åº“å’Œæƒ…ç»ªæ•°æ®åº“"""
        # åŠ è½½éŸ³é¢‘ç‰¹å¾æ•°æ®åº“
        features_db_file = self.features_dir / "features_database.json"
        if features_db_file.exists():
            try:
                with open(features_db_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                self.features_database = data.get('features_database', {})
                logger.info(f"âœ… åŠ è½½ç‰¹å¾æ•°æ®åº“: {len(self.features_database)} ä¸ªè§†é¢‘")
            except Exception as e:
                logger.error(f"åŠ è½½ç‰¹å¾æ•°æ®åº“å¤±è´¥: {e}")
        
        # æ„å»ºæƒ…ç»ªæ•°æ®åº“
        self._build_emotion_database()
    
    def _build_emotion_database(self):
        """æ„å»ºæƒ…ç»ªç‰¹å¾æ•°æ®åº“"""
        self.emotion_database = {}
        
        # è·å–æ‰€æœ‰æ”¯æŒçš„æƒ…ç»ª
        supported_emotions = music_feature_mapper.get_supported_emotions()
        
        for emotion in supported_emotions:
            # è·å–åŒ¹é…é˜¶æ®µç‰¹å¾ï¼ˆå¯¹åº”å‰25%è§†é¢‘å†…å®¹ï¼‰
            matching_features = music_feature_mapper.extract_matching_stage_features(emotion)
            
            # è½¬æ¢ä¸ºæ•°å€¼ç‰¹å¾å‘é‡
            feature_vector = self._emotion_features_to_vector(matching_features)
            
            self.emotion_database[emotion] = {
                'features': matching_features,
                'feature_vector': feature_vector,
                'normalized_vector': self._normalize_vector(feature_vector)
            }
        
        logger.info(f"âœ… æ„å»ºæƒ…ç»ªæ•°æ®åº“: {len(self.emotion_database)} ç§æƒ…ç»ª")
    
    def _emotion_features_to_vector(self, features: Dict[str, Any]) -> np.ndarray:
        """
        å°†æƒ…ç»ªç‰¹å¾è½¬æ¢ä¸ºæ•°å€¼å‘é‡
        
        Args:
            features: æƒ…ç»ªç‰¹å¾å­—å…¸
            
        Returns:
            np.ndarray: ç‰¹å¾å‘é‡
        """
        vector = []
        
        # èŠ‚æ‹ç‰¹å¾
        tempo_val = self._extract_tempo_value(features.get('tempo', ''))
        vector.append(tempo_val)
        
        # è°ƒæ€§ç‰¹å¾
        key_val = self._extract_key_value(features.get('key', ''))
        vector.append(key_val)
        
        # åŠ¨æ€ç‰¹å¾
        dynamics_val = self._extract_dynamics_value(features.get('dynamics', ''))
        vector.append(dynamics_val)
        
        # æƒ…ç»ªå¼ºåº¦ï¼ˆåŸºäºmoodæè¿°ï¼‰
        mood_intensity = self._extract_mood_intensity(features.get('mood', ''))
        vector.append(mood_intensity)
        
        # ä¹å™¨å¤æ‚åº¦
        instrument_complexity = self._extract_instrument_complexity(features.get('instrumental', ''))
        vector.append(instrument_complexity)
        
        # ç»‡ä½“å¤æ‚åº¦
        texture_complexity = self._extract_texture_complexity(features.get('texture', ''))
        vector.append(texture_complexity)
        
        return np.array(vector, dtype=np.float32)
    
    def _extract_tempo_value(self, tempo_desc: str) -> float:
        """ä»tempoæè¿°ä¸­æå–æ•°å€¼"""
        tempo_map = {
            'slow': 0.2, 'deeply': 0.1, 'profound': 0.1,
            'moderate': 0.5, 'gradually': 0.4, 'natural': 0.5,
            'fast': 0.8, 'urgent': 0.9, 'agitated': 0.85,
            'tired': 0.25, 'sluggish': 0.2, 'gentle': 0.3,
            'smooth': 0.4, 'flowing': 0.45, 'weightless': 0.15
        }
        
        for key, value in tempo_map.items():
            if key in tempo_desc.lower():
                return value
        
        return 0.5  # é»˜è®¤ä¸­ç­‰
    
    def _extract_key_value(self, key_desc: str) -> float:
        """ä»è°ƒæ€§æè¿°ä¸­æå–æ•°å€¼"""
        # ç®€åŒ–æ˜ å°„ï¼šminor=è´Ÿé¢æƒ…ç»ª(0.2-0.4), major=æ­£é¢æƒ…ç»ª(0.6-0.8), neutral=ä¸­æ€§(0.5)
        if 'minor' in key_desc.lower():
            if 'anxious' in key_desc or 'tense' in key_desc:
                return 0.2
            elif 'weary' in key_desc:
                return 0.25
            else:
                return 0.3
        elif 'major' in key_desc.lower():
            if 'calm' in key_desc:
                return 0.7
            elif 'warm' in key_desc:
                return 0.75
            else:
                return 0.6
        else:
            return 0.5  # neutral
    
    def _extract_dynamics_value(self, dynamics_desc: str) -> float:
        """ä»åŠ¨æ€æè¿°ä¸­æå–æ•°å€¼"""
        dynamics_map = {
            'whisper': 0.1, 'gentle': 0.2, 'soft': 0.25,
            'restless': 0.7, 'sharp': 0.8, 'heavy': 0.6,
            'compressed': 0.75, 'embracing': 0.3, 'peaceful': 0.2,
            'expanding': 0.5, 'free': 0.4
        }
        
        for key, value in dynamics_map.items():
            if key in dynamics_desc.lower():
                return value
        
        return 0.4  # é»˜è®¤
    
    def _extract_mood_intensity(self, mood_desc: str) -> float:
        """ä»æƒ…ç»ªæè¿°ä¸­æå–å¼ºåº¦"""
        intensity_keywords = {
            'deep': 0.8, 'profound': 0.9, 'transcendent': 1.0,
            'anxiety': 0.7, 'stress': 0.8, 'overload': 0.9,
            'exhausted': 0.6, 'fatigue': 0.5, 'tired': 0.4,
            'irritated': 0.7, 'angry': 0.8, 'calm': 0.3,
            'peace': 0.2, 'tranquil': 0.25, 'serene': 0.2
        }
        
        max_intensity = 0.5
        for keyword, intensity in intensity_keywords.items():
            if keyword in mood_desc.lower():
                max_intensity = max(max_intensity, intensity)
        
        return max_intensity
    
    def _extract_instrument_complexity(self, instrument_desc: str) -> float:
        """ä»ä¹å™¨æè¿°ä¸­æå–å¤æ‚åº¦"""
        if not instrument_desc:
            return 0.5
        
        # ç»Ÿè®¡ä¹å™¨æ•°é‡å’Œç±»å‹
        instruments = instrument_desc.lower().split(',')
        complexity = len(instruments) * 0.2  # åŸºç¡€å¤æ‚åº¦
        
        # ç‰¹å®šä¹å™¨çš„å¤æ‚åº¦åŠ æƒ
        complex_instruments = ['percussion', 'brass', 'strings']
        simple_instruments = ['piano', 'ambient', 'pads']
        
        for inst in instruments:
            inst = inst.strip()
            if any(ci in inst for ci in complex_instruments):
                complexity += 0.3
            elif any(si in inst for si in simple_instruments):
                complexity += 0.1
        
        return min(complexity, 1.0)
    
    def _extract_texture_complexity(self, texture_desc: str) -> float:
        """ä»ç»‡ä½“æè¿°ä¸­æå–å¤æ‚åº¦"""
        texture_map = {
            'minimal': 0.1, 'simple': 0.2, 'spacious': 0.2,
            'complex': 0.8, 'layered': 0.7, 'polyphonic': 0.9,
            'angular': 0.6, 'flowing': 0.4, 'smooth': 0.3,
            'balanced': 0.5, 'compressed': 0.7, 'intense': 0.8
        }
        
        for key, value in texture_map.items():
            if key in texture_desc.lower():
                return value
        
        return 0.5
    
    def _normalize_vector(self, vector: np.ndarray) -> np.ndarray:
        """å½’ä¸€åŒ–å‘é‡"""
        norm = np.linalg.norm(vector)
        if norm > 0:
            return vector / norm
        return vector
    
    def _audio_features_to_vector(self, features: Dict[str, Any]) -> np.ndarray:
        """
        å°†éŸ³é¢‘ç‰¹å¾è½¬æ¢ä¸ºå‘é‡
        
        Args:
            features: éŸ³é¢‘ç‰¹å¾å­—å…¸
            
        Returns:
            np.ndarray: ç‰¹å¾å‘é‡
        """
        vector = []
        
        # æ ‡å‡†åŒ–ç‰¹å¾æå–
        vector.append(min(features.get('tempo_estimate', 0) / 200.0, 1.0))  # èŠ‚æ‹(0-200 BPM)
        vector.append(min(features.get('rms_energy', 0) * 10, 1.0))         # èƒ½é‡
        vector.append(min(features.get('brightness', 0), 1.0))              # äº®åº¦
        vector.append(min(features.get('warmth', 0), 1.0))                  # æ¸©æš–åº¦
        vector.append(min(features.get('rhythm_regularity', 0), 1.0))       # èŠ‚å¥è§„å¾‹æ€§
        vector.append(min(features.get('spectral_centroid', 0) / 8000.0, 1.0))  # é¢‘è°±è´¨å¿ƒ
        vector.append(min(features.get('dynamic_range', 0) / 2.0, 1.0))     # åŠ¨æ€èŒƒå›´
        
        return np.array(vector, dtype=np.float32)
    
    def calculate_similarity(self, 
                           emotion: str, 
                           video_features: Dict[str, Any]) -> float:
        """
        è®¡ç®—æƒ…ç»ªä¸è§†é¢‘ç‰¹å¾çš„ç›¸ä¼¼åº¦
        
        Args:
            emotion: ç›®æ ‡æƒ…ç»ª
            video_features: è§†é¢‘çš„éŸ³é¢‘ç‰¹å¾
            
        Returns:
            float: ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        if emotion not in self.emotion_database:
            logger.warning(f"æœªæ‰¾åˆ°æƒ…ç»ª '{emotion}' çš„ç‰¹å¾æ•°æ®")
            return 0.0
        
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰CLAMP3ç‰¹å¾
            if 'clamp3_features' in video_features or 'feature_vector' in video_features:
                return self._calculate_clamp3_similarity(emotion, video_features)
            else:
                return self._calculate_traditional_similarity(emotion, video_features)
                
        except Exception as e:
            logger.error(f"ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_clamp3_similarity(self, emotion: str, video_features: Dict[str, Any]) -> float:
        """
        ä½¿ç”¨CLAMP3ç‰¹å¾è®¡ç®—ç›¸ä¼¼åº¦
        
        Args:
            emotion: ç›®æ ‡æƒ…ç»ª
            video_features: åŒ…å«CLAMP3ç‰¹å¾çš„è§†é¢‘ç‰¹å¾
            
        Returns:
            float: ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        # è·å–CLAMP3ç‰¹å¾å‘é‡
        clamp3_vector = video_features.get('clamp3_features') or video_features.get('feature_vector')
        
        if clamp3_vector is None:
            logger.warning("CLAMP3ç‰¹å¾å‘é‡ä¸ºç©º")
            return 0.0
        
        # ç¡®ä¿æ˜¯numpyæ•°ç»„
        if not isinstance(clamp3_vector, np.ndarray):
            clamp3_vector = np.array(clamp3_vector)
        
        # å¦‚æœæ˜¯2Dæ•°ç»„ï¼Œå–ç¬¬ä¸€ä¸ªç»´åº¦
        if clamp3_vector.ndim > 1:
            clamp3_vector = clamp3_vector.flatten()
        
        # è·å–æƒ…ç»ªç‰¹å¾å‘é‡
        emotion_vector = self.emotion_database[emotion]['normalized_vector']
        
        # å¯¹äºCLAMP3ç‰¹å¾ï¼Œæˆ‘ä»¬éœ€è¦å…ˆå°†å…¶æ˜ å°„åˆ°æƒ…ç»ªç©ºé—´
        # è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„æ–¹æ³•ï¼šåŸºäºCLAMP3ç‰¹å¾çš„ç»Ÿè®¡ç‰¹æ€§
        
        # è®¡ç®—CLAMP3ç‰¹å¾çš„ç»Ÿè®¡ç‰¹æ€§
        clamp3_stats = self._extract_clamp3_statistics(clamp3_vector)
        
        # å°†ç»Ÿè®¡ç‰¹æ€§æ˜ å°„åˆ°æƒ…ç»ªç‰¹å¾ç©ºé—´
        mapped_audio_vector = self._map_clamp3_to_emotion_space(clamp3_stats)
        mapped_audio_vector_norm = self._normalize_vector(mapped_audio_vector)
        
        # ç¡®ä¿å‘é‡é•¿åº¦ä¸€è‡´
        min_len = min(len(emotion_vector), len(mapped_audio_vector_norm))
        emotion_vec = emotion_vector[:min_len]
        audio_vec = mapped_audio_vector_norm[:min_len]
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        dot_product = np.dot(emotion_vec, audio_vec)
        cosine_similarity = max(0.0, dot_product)  # ç¡®ä¿éè´Ÿ
        
        # è®¡ç®—æ¬§å‡ é‡Œå¾—è·ç¦»ç›¸ä¼¼åº¦
        euclidean_distance = np.linalg.norm(emotion_vec - audio_vec)
        euclidean_similarity = 1.0 / (1.0 + euclidean_distance)
        
        # åŠ æƒç»„åˆï¼ˆCLAMP3ç‰¹å¾æ›´ä¾èµ–ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        final_similarity = 0.8 * cosine_similarity + 0.2 * euclidean_similarity
        
        return float(final_similarity)
    
    def _extract_clamp3_statistics(self, clamp3_vector: np.ndarray) -> Dict[str, float]:
        """
        ä»CLAMP3ç‰¹å¾å‘é‡ä¸­æå–ç»Ÿè®¡ç‰¹æ€§
        
        Args:
            clamp3_vector: CLAMP3ç‰¹å¾å‘é‡
            
        Returns:
            Dict: ç»Ÿè®¡ç‰¹æ€§
        """
        stats = {}
        
        # åŸºç¡€ç»Ÿè®¡é‡
        stats['mean'] = float(np.mean(clamp3_vector))
        stats['std'] = float(np.std(clamp3_vector))
        stats['max'] = float(np.max(clamp3_vector))
        stats['min'] = float(np.min(clamp3_vector))
        stats['median'] = float(np.median(clamp3_vector))
        
        # åˆ†ä½æ•°ç‰¹å¾
        stats['q25'] = float(np.percentile(clamp3_vector, 25))
        stats['q75'] = float(np.percentile(clamp3_vector, 75))
        stats['iqr'] = stats['q75'] - stats['q25']
        
        # å½¢çŠ¶ç‰¹å¾
        stats['skewness'] = float(self._calculate_skewness(clamp3_vector))
        stats['kurtosis'] = float(self._calculate_kurtosis(clamp3_vector))
        
        # èƒ½é‡ç‰¹å¾
        stats['energy'] = float(np.sum(clamp3_vector ** 2))
        stats['rms'] = float(np.sqrt(np.mean(clamp3_vector ** 2)))
        
        # é¢‘åŸŸç‰¹å¾ï¼ˆå°†CLAMP3ç‰¹å¾è§†ä¸ºæ—¶é—´åºåˆ—ï¼‰
        fft_features = np.fft.fft(clamp3_vector)
        magnitude = np.abs(fft_features)
        stats['spectral_centroid'] = float(np.sum(np.arange(len(magnitude)) * magnitude) / np.sum(magnitude))
        
        return stats
    
    def _calculate_skewness(self, data: np.ndarray) -> float:
        """è®¡ç®—ååº¦"""
        if len(data) < 3:
            return 0.0
        mean = np.mean(data)
        std = np.std(data)
        if std == 0:
            return 0.0
        return np.mean(((data - mean) / std) ** 3)
    
    def _calculate_kurtosis(self, data: np.ndarray) -> float:
        """è®¡ç®—å³°åº¦"""
        if len(data) < 4:
            return 0.0
        mean = np.mean(data)
        std = np.std(data)
        if std == 0:
            return 0.0
        return np.mean(((data - mean) / std) ** 4) - 3
    
    def _map_clamp3_to_emotion_space(self, clamp3_stats: Dict[str, float]) -> np.ndarray:
        """
        å°†CLAMP3ç»Ÿè®¡ç‰¹æ€§æ˜ å°„åˆ°æƒ…ç»ªç‰¹å¾ç©ºé—´
        
        Args:
            clamp3_stats: CLAMP3ç»Ÿè®¡ç‰¹æ€§
            
        Returns:
            np.ndarray: æ˜ å°„åçš„æƒ…ç»ªç©ºé—´ç‰¹å¾å‘é‡
        """
        # åˆ›å»ºæ˜ å°„å‘é‡ï¼ˆä¸æƒ…ç»ªç‰¹å¾å‘é‡ç›¸åŒé•¿åº¦ï¼‰
        mapped_vector = []
        
        # 1. èŠ‚æ‹ç‰¹å¾ï¼ˆåŸºäºèƒ½é‡å’Œå˜åŒ–ï¼‰
        tempo_proxy = min(abs(clamp3_stats['energy']) / 10.0, 1.0)
        mapped_vector.append(tempo_proxy)
        
        # 2. è°ƒæ€§ç‰¹å¾ï¼ˆåŸºäºé¢‘è°±è´¨å¿ƒï¼‰
        key_proxy = min(abs(clamp3_stats['spectral_centroid']) / 100.0, 1.0)
        mapped_vector.append(key_proxy)
        
        # 3. åŠ¨æ€ç‰¹å¾ï¼ˆåŸºäºæ ‡å‡†å·®ï¼‰
        dynamics_proxy = min(clamp3_stats['std'] * 2.0, 1.0)
        mapped_vector.append(dynamics_proxy)
        
        # 4. æƒ…ç»ªå¼ºåº¦ï¼ˆåŸºäºRMSï¼‰
        intensity_proxy = min(clamp3_stats['rms'] * 5.0, 1.0)
        mapped_vector.append(intensity_proxy)
        
        # 5. å¤æ‚åº¦ï¼ˆåŸºäºååº¦ï¼‰
        complexity_proxy = min(abs(clamp3_stats['skewness']) / 2.0, 1.0)
        mapped_vector.append(complexity_proxy)
        
        # 6. ç»‡ä½“ç‰¹å¾ï¼ˆåŸºäºå³°åº¦ï¼‰
        texture_proxy = min(abs(clamp3_stats['kurtosis']) / 5.0, 1.0)
        mapped_vector.append(texture_proxy)
        
        return np.array(mapped_vector, dtype=np.float32)
    
    def _calculate_traditional_similarity(self, emotion: str, video_features: Dict[str, Any]) -> float:
        """
        ä½¿ç”¨ä¼ ç»Ÿç‰¹å¾è®¡ç®—ç›¸ä¼¼åº¦
        
        Args:
            emotion: ç›®æ ‡æƒ…ç»ª
            video_features: ä¼ ç»ŸéŸ³é¢‘ç‰¹å¾
            
        Returns:
            float: ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        # è·å–æƒ…ç»ªç‰¹å¾å‘é‡
        emotion_vector = self.emotion_database[emotion]['normalized_vector']
        
        # è½¬æ¢éŸ³é¢‘ç‰¹å¾ä¸ºå‘é‡
        audio_vector = self._audio_features_to_vector(video_features)
        audio_vector_norm = self._normalize_vector(audio_vector)
        
        # ç¡®ä¿å‘é‡é•¿åº¦ä¸€è‡´
        min_len = min(len(emotion_vector), len(audio_vector_norm))
        emotion_vec = emotion_vector[:min_len]
        audio_vec = audio_vector_norm[:min_len]
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        dot_product = np.dot(emotion_vec, audio_vec)
        cosine_similarity = max(0.0, dot_product)  # ç¡®ä¿éè´Ÿ
        
        # è®¡ç®—æ¬§å‡ é‡Œå¾—è·ç¦»ç›¸ä¼¼åº¦
        euclidean_distance = np.linalg.norm(emotion_vec - audio_vec)
        euclidean_similarity = 1.0 / (1.0 + euclidean_distance)
        
        # åŠ æƒç»„åˆ
        final_similarity = 0.7 * cosine_similarity + 0.3 * euclidean_similarity
        
        return float(final_similarity)
    
    def retrieve_videos(self, 
                       emotion: str, 
                       top_k: int = 5,
                       min_similarity: float = 0.1) -> List[Tuple[str, float, Dict[str, Any]]]:
        """
        æ£€ç´¢æœ€åŒ¹é…çš„è§†é¢‘
        
        Args:
            emotion: ç›®æ ‡æƒ…ç»ª
            top_k: è¿”å›top-kç»“æœ
            min_similarity: æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            List[Tuple]: (è§†é¢‘è·¯å¾„, ç›¸ä¼¼åº¦åˆ†æ•°, è§†é¢‘ä¿¡æ¯) çš„åˆ—è¡¨
        """
        if not self.features_database:
            logger.error("ç‰¹å¾æ•°æ®åº“ä¸ºç©ºï¼Œè¯·å…ˆæå–è§†é¢‘ç‰¹å¾")
            return []
        
        logger.info(f"æ£€ç´¢æƒ…ç»ª '{emotion}' çš„åŒ¹é…è§†é¢‘ (top-{top_k})")
        
        similarities = []
        
        for video_path, video_features in self.features_database.items():
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarity = self.calculate_similarity(emotion, video_features)
            
            if similarity >= min_similarity:
                similarities.append((video_path, similarity, video_features))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # è¿”å›top-kç»“æœ
        top_results = similarities[:top_k]
        
        logger.info(f"æ‰¾åˆ° {len(similarities)} ä¸ªåŒ¹é…è§†é¢‘ï¼Œè¿”å›top-{len(top_results)}")
        
        for i, (path, score, _) in enumerate(top_results, 1):
            logger.info(f"  {i}. {Path(path).name}: {score:.3f}")
        
        return top_results
    
    def get_random_from_top_k(self, 
                             emotion: str, 
                             top_k: int = 5) -> Optional[Tuple[str, float, Dict[str, Any]]]:
        """
        ä»top-kç»“æœä¸­éšæœºé€‰æ‹©ä¸€ä¸ª
        
        Args:
            emotion: ç›®æ ‡æƒ…ç»ª
            top_k: top-kèŒƒå›´
            
        Returns:
            Tuple: (è§†é¢‘è·¯å¾„, ç›¸ä¼¼åº¦åˆ†æ•°, è§†é¢‘ä¿¡æ¯) æˆ– None
        """
        top_results = self.retrieve_videos(emotion, top_k)
        
        if not top_results:
            logger.warning(f"æ²¡æœ‰æ‰¾åˆ°åŒ¹é…æƒ…ç»ª '{emotion}' çš„è§†é¢‘")
            return None
        
        # éšæœºé€‰æ‹©ä¸€ä¸ª
        selected = random.choice(top_results)
        
        logger.info(f"ä»top-{len(top_results)}ä¸­éšæœºé€‰æ‹©: {Path(selected[0]).name} (ç›¸ä¼¼åº¦: {selected[1]:.3f})")
        
        return selected
    
    def get_retrieval_stats(self) -> Dict[str, Any]:
        """è·å–æ£€ç´¢ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_videos': len(self.features_database),
            'supported_emotions': len(self.emotion_database),
            'emotion_list': list(self.emotion_database.keys()),
            'feature_weights': self.feature_weights,
            'emotion_weights': self.emotion_weights,
            'last_updated': datetime.now().isoformat()
        }
    
    def update_feature_weights(self, new_weights: Dict[str, float]):
        """æ›´æ–°ç‰¹å¾æƒé‡"""
        self.feature_weights.update(new_weights)
        logger.info(f"æ›´æ–°ç‰¹å¾æƒé‡: {new_weights}")
    
    def rebuild_emotion_database(self):
        """é‡å»ºæƒ…ç»ªæ•°æ®åº“"""
        logger.info("é‡å»ºæƒ…ç»ªæ•°æ®åº“...")
        self._build_emotion_database()

class TherapyVideoSelector:
    """
    ç–—æ„ˆè§†é¢‘é€‰æ‹©å™¨
    åŸºäºç”¨æˆ·è¾“å…¥å’Œæƒ…ç»ªçŠ¶æ€é€‰æ‹©æœ€é€‚åˆçš„ç–—æ„ˆè§†é¢‘
    """
    
    def __init__(self, retrieval_engine: VideoRetrievalEngine):
        """
        åˆå§‹åŒ–è§†é¢‘é€‰æ‹©å™¨
        
        Args:
            retrieval_engine: æ£€ç´¢å¼•æ“å®ä¾‹
        """
        self.retrieval_engine = retrieval_engine
        self.selection_history = []
    
    def select_therapy_video(self, 
                           user_input: str,
                           duration_preference: Optional[int] = None) -> Optional[Dict[str, Any]]:
        """
        é€‰æ‹©ç–—æ„ˆè§†é¢‘
        
        Args:
            user_input: ç”¨æˆ·æƒ…ç»ªè¾“å…¥
            duration_preference: æ—¶é•¿åå¥½ï¼ˆåˆ†é’Ÿï¼‰
            
        Returns:
            Dict: é€‰æ‹©çš„è§†é¢‘ä¿¡æ¯
        """
        # 1. æƒ…ç»ªè¯†åˆ«
        emotion, confidence = emotion_recognizer.detect_emotion(user_input)
        
        logger.info(f"æƒ…ç»ªè¯†åˆ«: {emotion} (ç½®ä¿¡åº¦: {confidence:.2%})")
        
        # 2. æ£€ç´¢åŒ¹é…è§†é¢‘
        selected_result = self.retrieval_engine.get_random_from_top_k(emotion, top_k=5)
        
        if not selected_result:
            return None
        
        video_path, similarity, video_features = selected_result
        
        # 3. æ„å»ºè¿”å›ä¿¡æ¯
        video_info = {
            'video_path': video_path,
            'video_name': Path(video_path).name,
            'detected_emotion': emotion,
            'emotion_confidence': confidence,
            'similarity_score': similarity,
            'selected_at': datetime.now().isoformat(),
            'user_input': user_input,
            'video_features': video_features,
            'therapy_stage': 'ISOåŒ¹é…é˜¶æ®µ',
            'therapy_duration': video_features.get('duration', 0)
        }
        
        # 4. è®°å½•é€‰æ‹©å†å²
        self.selection_history.append(video_info)
        
        # 5. è·å–ISOä¸‰é˜¶æ®µç‰¹å¾
        iso_features = music_feature_mapper.get_music_features(emotion)
        video_info['iso_features'] = iso_features
        
        logger.info(f"âœ… é€‰æ‹©ç–—æ„ˆè§†é¢‘: {video_info['video_name']}")
        logger.info(f"   æƒ…ç»ª: {emotion} | ç›¸ä¼¼åº¦: {similarity:.3f} | æ—¶é•¿: {video_info['therapy_duration']:.1f}ç§’")
        
        return video_info
    
    def get_selection_history(self) -> List[Dict[str, Any]]:
        """è·å–é€‰æ‹©å†å²"""
        return self.selection_history.copy()
    
    def clear_history(self):
        """æ¸…ç©ºé€‰æ‹©å†å²"""
        self.selection_history = []
        logger.info("é€‰æ‹©å†å²å·²æ¸…ç©º")

if __name__ == "__main__":
    # æµ‹è¯•æ£€ç´¢å¼•æ“
    engine = VideoRetrievalEngine()
    
    print("ğŸ” æµ‹è¯•æ£€ç´¢å¼•æ“:")
    print(f"ç‰¹å¾æ•°æ®åº“: {len(engine.features_database)} ä¸ªè§†é¢‘")
    print(f"æƒ…ç»ªæ•°æ®åº“: {len(engine.emotion_database)} ç§æƒ…ç»ª")
    
    # æµ‹è¯•æ£€ç´¢
    test_emotion = "ç„¦è™‘"
    results = engine.retrieve_videos(test_emotion, top_k=3)
    
    print(f"\nğŸ¯ æ£€ç´¢æµ‹è¯• - æƒ…ç»ª: {test_emotion}")
    for i, (path, score, _) in enumerate(results, 1):
        print(f"  {i}. {Path(path).name}: {score:.3f}")
    
    # æµ‹è¯•ç–—æ„ˆè§†é¢‘é€‰æ‹©å™¨
    print(f"\nğŸŒ™ æµ‹è¯•ç–—æ„ˆè§†é¢‘é€‰æ‹©å™¨:")
    selector = TherapyVideoSelector(engine)
    
    test_inputs = [
        "æˆ‘æ„Ÿåˆ°å¾ˆç„¦è™‘ï¼Œå¿ƒè·³åŠ é€Ÿ",
        "å¤ªç´¯äº†ï¼Œèº«ä½“å’Œç²¾ç¥éƒ½å¾ˆç–²æƒ«",
        "æ¯”è¾ƒå¹³é™ï¼Œä½†å¸Œæœ›æ›´æ·±å±‚çš„æ”¾æ¾"
    ]
    
    for user_input in test_inputs:
        result = selector.select_therapy_video(user_input)
        if result:
            print(f"è¾“å…¥: {user_input}")
            print(f"é€‰æ‹©: {result['video_name']} | {result['detected_emotion']} | {result['similarity_score']:.3f}")
            print("-" * 50)